/*******************************************************************************
 * Copyright (c) 2010 Haifeng Li
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *******************************************************************************/
package smile.clustering

import scala.util.Random
import scala.util.control.Breaks._
import smile.math.distance.{Distance, Hamming}

/** K-Modes clustering. K-Modes is the binary equivalent for K-Means.
	* The mean update for centroids is replace by the mode one which is
	* a majority vote among element of each cluster.
	*
  * Complexity is O(k.t.n) with Hamming distance, quadratic with other metrics.
	*
	* <h2>References</h2>
  *  - Joshua Zhexue Huang. Clustering Categorical Data with k-Modes.
	*    http://www.irma-international.org/viewtitle/10828/
  *  - Antara Prakash, et al. Review on K-Mode Clustering.
  *    https://www.ijecs.in/index.php/ijecs/article/download/3058/2834/
  *  - Python implementations of the k-modes and k-prototypes clustering algorithms for clustering categorical data.
	*    https://pypi.python.org/pypi/kmodes/
	*
  * @param data the dataset
  * @param k number of clusters
  * @param epsilon distance between ancient modes and update modes under which we consider than the algorithm have converged, if and only if all every mode have converged
  * @param maxIter number maximal of iteration
  * @param metric the dissimilarity measure used
	*
	* @author Beck GaÃ«l
  */
class KModes(data: Array[Array[Int]], k: Int, epsilon: Double, maxIter: Int = 100, metric: Distance[Array[Int]] = new Hamming()) extends PartitionClustering[Array[Int]] with Serializable {

	/** Dimension */
	val p = data.head.size

	/**
		* The modes of each cluster.
		*/
	val modes = Array.fill(k) {
	  Array.fill(p) { Random.nextInt(2) }
	}

	/**
		* The cluster labels of data.
		*/
	y = Array.ofDim[Int](data.length)

	private var distortion: Double = Double.MaxValue

	/**
		* The distance between data and mode.
		*/
	private val dist: Array[Double] = Array.ofDim[Double](data.length)

	/** The size of each cluster. */
	size = Array.ofDim[Int](k)

	breakable { for (iter <- 0 until maxIter) {
		var wcss = 0.0

		for (i <- 0 until data.length) {
			val (d, c) = nearest(data(i))
			y(i) = c
			dist(i) = d
			wcss += d
		}

		modes.foreach { mode => java.util.Arrays.fill(mode, 0) }
		java.util.Arrays.fill(size, 0)

		// Fast way if we use Hamming distance
		if (metric.isInstanceOf[Hamming]) {
			for (i <- 0 until data.length) {
				for (j <- 0 until p) modes(y(i))(j) += data(i)(j)
				size(y(i)) += 1
			}

			for (i <- 0 until modes.length) {
				val mode = modes(i)
				for (j <- 0 until mode.length) {
					if (mode(j) * 2 >= size(i)) mode(j) = 1 else mode(j) = 0
				}
			}
		} else {
			// search mode that is the data point closest to centroid
			dist.zipWithIndex.zip(y).groupBy{ case (_, cluster) => cluster }.foreach { case (cluster, aggregates) =>
				val mode = aggregates.minBy(_._1)._2
				modes(cluster) = data(mode)
			}
		}

		if (distortion <= wcss) break else distortion = wcss
	} }

	/**
		* Return the nearest mode and distance for a specific point.
		**/
	private def nearest(x: Array[Int]): (Double, Int) = {
		modes.map { mode => metric.d(mode, x) }.zipWithIndex.minBy(_._1)
	}

	/**
		* Return the nearest mode for a specific point.
		**/
	override def predict(x: Array[Int]): Int = {
		nearest(x)._2
	}
}
