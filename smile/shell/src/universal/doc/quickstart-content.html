<div class="col-md-3 col-md-push-9 hidden-xs hidden-sm">
    <div id="sidebar">
        <div class="sidebar-toc" style="margin-bottom: 20px;">
            <p class="toc-header">Contents</p>
            <div id="toc"></div>
        </div>
        <div id="search">
            <script>
                (function() {
                    var cx = '010264411143030149390:ajvee_ckdzs';
                    var gcse = document.createElement('script');
                    gcse.type = 'text/javascript';
                    gcse.async = true;
                    gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') +
                            '//cse.google.com/cse.js?cx=' + cx;
                    var s = document.getElementsByTagName('script')[0];
                    s.parentNode.insertBefore(gcse, s);
                })();
            </script>
            <gcse:searchbox-only></gcse:searchbox-only>
        </div>
    </div>
</div>

<div class="col-md-9 col-md-pull-3">
    <h1 id="quickstart-top" class="title">Quick Start</h1>

    <p>Smile is a fast and comprehensive machine learning system.
        With advanced data structures and algorithms, Smile delivers the state-of-art performance.
        Smile is self contained and requires only Java standard library.
        Since v1.4, Smile may optionally leverage native BLAS/LAPACK library too.
        It also provides high-level operators in Scala and an interactive shell.
        In practice, data scientists usually build models with high-level tools such as R, Matlab,
        SAS, etc. However, developers have to spend a lot of time and energy to incorporate these
        models in the production system that are often implemented in general purpose programming
        languages such as Java and Scala. With Smile, data scientists and developers can work
        in the same environment to build machine learning applications quickly!</p>

    <h2 id="download">Download</h2>

    <p>Get Smile from the <a href="https://github.com/haifengl/smile/releases">releases page</a> of
        the project website. We provide pre-packaged binary for Mac. The universal tarball
        is also available and can be used on Windows and Linux.</p>

    <p>If you would like to build Smile from source, please first install Java 8, Scala 2.12
        and SBT 0.13+.
        Smile uses <a href="http://www.scala-sbt.org/sbt-pgp/">sbt-pgp</a> plugin when
        publishing to maven central repository. Although you will not publish the project,
        you still have to set up sbt-pgp in order to build the packages. Fortunately, it is
        very easy. Simply add the following to your <code>~/.sbt/0.13/plugins/gpg.sbt</code> file:</p>

    <pre class="prettyprint lang-scala"><code>
    addSbtPlugin("com.jsuereth" % "sbt-pgp" % "1.0.0")
    </code></pre>

    <p>Then clone the repo and build the package:</p>

    <pre class="prettyprint lang-bash"><code>
    git clone https://github.com/haifengl/smile.git
    cd smile
    sbt package
    </code></pre>

    <p>To build with Scala 2.11, run</p>

    <pre class="prettyprint lang-bash"><code>
    sbt ++2.11.8 scala/package
    </code></pre>

    <p>To build with Scala 2.10, run</p>

    <pre class="prettyprint lang-bash"><code>
    sbt ++2.10.6 scala/package
    </code></pre>

    <p>To test the latest code, run the following</p>

    <pre class="prettyprint lang-bash"><code>
    git pull
    ./smile.sh
    </code></pre>

    <p>which will build the system and enter the shell.</p>

    <p>Smile runs on both Windows and UNIX-like systems (e.g. Linux, Mac OS).
        All you need is to have <code>Java 8</code> installed on your system <code>PATH</code>,
        or the <code>JAVA_HOME</code> environment variable pointing to a Java installation.</p>

    <h2 id="shell">Shell</h2>

    <p>Smile comes with an interactive shell. In the home directory of Smile, type</p>

    <pre class="prettyprint lang-bash"><code>
    ./bin/smile
    </code></pre>

    <p>to enter the shell, which is based on <a href="http://ammonite.io">Ammonite REPL</a>.
        In the shell, you can run any valid Scala expressions.
        In the simplest case, you can use it as a calculator.
        Besides, all high-level Smile operators are predefined
        in the shell. Be default, the shell uses up to 4GB memory. If you need more memory
        to handle large data, use the option <code>-J-Xmx</code>. For example,</p>

    <pre class="prettyprint lang-bash"><code>
    ./bin/smile -J-Xmx8192M
    </code></pre>

    <p>You can also modify the configuration file <code>./conf/application.ini</code>
        for the memory and other JVM settings. On Windows, the configuration
        file is <code>smile_config.txt</code>.</p>

    <p>To get help information of Smile high-level operators,
        the <code>help</code> in the shell. You can also get detailed information on
        each operator by typing <code>help("command")</code>, e.g.
        <code>help("svm")</code>. To exit the shell, type <code>exit</code>.</p>

    <p>In the shell, type <code>demo</code> to bring up the demo window,
        which shows off various Smile's machine learning capabilities.</p>

    <p>You can also type <code>benchmark()</code> to see Smile's performance
        on a couple of test data. You can run a particular benchmark by
        <code>bencharm("test name")</code>, where test name could be "airline",
        "usps", etc.</p>

    <p>In the <code>data</code> directory, we also include many open datasets,
        which are frequently used in research and benchmark. Now let&#8217;s build
        a classification model with Smile. It is as easy as</p>

    <pre class="prettyprint lang-scala"><code>
    val data = read.arff("data/weka/iris.arff", 4)
    val (x, y) = data.unzipInt

    val rf = randomForest(x, y)
    println(s"OOB error = ${rf.error}")
    rf.predict(x(0))
    </code></pre>

    <p>In this example, we use the famous Iris data from R.A. Fisher. The data
        is in Weka's ARFF format. The second parameter of read.arff is the column index
        of response variable. With our parsers, the column index starts with 0. The
        function read.arff returns an object of <code>AttributeDataset</code>.
        Besides the data itself, an <code>AttributeDataset</code> object also contains many meta data.
        Then we use the help function <code>unzipInt</code> to get the training
        data and labels. For regression, you may use <code>unzipDouble</code> as
        the response variable is real value. Finally, we train a random forest
        with default parameters and print out its OOB (out of bag) error. We can apply
        the model on new data samples with the method <code>predict</code>.</p>

    <div id="btnv">
        <a class="btn-next-text" href="overview.html" title="Next Section: Overview"><span>Overview</span></a>
        <span class="btn-arrow-right">&nbsp;&rarr;</span>
    </div>
</div>

<script type="text/javascript">
    $('#toc').toc({exclude: 'h1, h5, h6', context: '', autoId: true, numerate: false});
</script>