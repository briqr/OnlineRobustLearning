<!-- scroll/follow sidebar -->
<script src="js/follow-sidebar.js" type="text/javascript"></script>

<div class="col-md-3 col-md-push-9 hidden-xs hidden-sm">
    <div id="sidebar">
        <div class="sidebar-toc" style="margin-bottom: 20px;">
            <p class="toc-header">Contents</p>
            <div id="toc"></div>
        </div>
        <div id="search">
            <script>
                (function() {
                    var cx = '010264411143030149390:ajvee_ckdzs';
                    var gcse = document.createElement('script');
                    gcse.type = 'text/javascript';
                    gcse.async = true;
                    gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') +
                            '//cse.google.com/cse.js?cx=' + cx;
                    var s = document.getElementsByTagName('script')[0];
                    s.parentNode.insertBefore(gcse, s);
                })();
            </script>
            <gcse:searchbox-only></gcse:searchbox-only>
        </div>
    </div>
</div>

<div class="col-md-9 col-md-pull-3">
    <h1 id="data-top" class="title">Data</h1>

    <p>Machine learning is all about building models from data. However, data scientists
        frequently talk about models and algorithms first, which very likely generates
        suboptimal results. The other approach is to play with the data first. Even simple
        statistics and plots can help us get feelings of data and problems, which more likely
        lead us to better modelling.</p>

    <h2 id="features">Features</h2>

    <p>A feature is an individual measurable property of a phenomenon being observed.
        Features are also called explanatory variables, independent variables, predictors, regressors, etc.
        Any attribute could be a feature, but choosing informative, discriminating and
        independent features is a crucial step for effective algorithms in machine learning.
        Features are usually numeric and a set of numeric features can be conveniently
        described by a feature vector. Structural features such as strings, sequences and
        graphs are also used in areas such as natural language processing, computational biology, etc.</p>

    <p>Feature engineering is the process of using domain knowledge of the data to create features that make
        machine learning algorithms work. Feature engineering is fundamental to the application of machine
        learning, and is both difficult and expensive. It requires the experimentation of multiple
        possibilities and the combination of automated
        techniques with the intuition and knowledge of the domain expert.</p>

    <h2 title="data-type">Data Type</h2>
    <p>Generally speaking, there are two major types of attributes:</p>
        <dl>
            <dt>Qualitative variables:</dt>
            <dd><p>The data values are non-numeric categories. Examples: Blood type, Gender.</p></dd>
            <dt>Quantitative variables:</dt>
            <dd><p>The data values are counts or numerical measurements. A quantitative
                variable can be either discrete such as the number of students receiving
                an 'A' in a class, or continuous such as GPA, salary and so on.</p></dd>
        </dl>

    <p>Another way of classifying data is by the measurement scales. In statistics,
        there are four generally used measurement scales:</p>

        <dl>
            <dt>Nominal data:</dt>
            <dd><p>Data values are non-numeric group labels. For example, Gender variable
                can be defined as male = 0 and female =1.</p></dd>
            <dt>Ordinal data:</dt>
            <dd><p>Data values are categorical and may be ranked in some numerically
                meaningful way. For example, strongly disagree to strong agree may be
                defined as 1 to 5.</p></dd>
            <dt>Continuous data:</dt>
            <dd><ul>
                <li><strong>Interval data:</strong>
                Data values are ranged in a real interval, which can be as large as
                from negative infinity to positive infinity. The difference between two
                values are meaningful, however, the ratio of two interval data is not
                meaningful. For example temperature, IQ. </li>
                <li><strong>Ratio data:</strong>
                Both difference and ratio of two values are meaningful. For example,
                salary, weight.</li>
            </ul></dd>
        </dl>

    <p>Smile has four attribute types:
        <a href="api/java/smile/data/NominalAttribute.html">NominalAttribute</a>,
        <a href="api/java/smile/data/NumericAttribute.html">NumericAttribute</a>,
        <a href="api/java/smile/data/DateAttribute.html">DateAttribute</a>,
        and <a href="api/java/smile/data/StringAttribute.html">StringAttribute</a>. Many
        machine learning algorithms can only handle numeric attributes while a few
        such as decision trees can process nominal attribute directly. Date attribute
        is useful in plotting. With some feature engineering, values like day of week
        can be used as nominal attribute. String attribute could be used in text mining
        and natural language processing.</p>

    <h2 id="AttributeDataset">AttributeDataset</h2>

    <p>Most Smile algorithms take simple <code>double[]</code> as input. But we often use
        the encapsulation class <a href="api/java/smile/data/AttributeDataset.html">AttributeDataset</a>.
        In fact, the output of most Smile data parsers is an AttributeDataset object.
        AttributeDataset contains a fixed number of attributes. All attribute values are stored as <code>double</code>
        even if the attribute may be nominal, ordinal, string, or date. The dataset is stored row-wise
        internally, which is fast for frequently accessing instances of dataset.</p>

    <p>A data object may have an associated class label (for classification) or real-valued
        response value (for regression). Optionally, a data object or attribute
        may have a (positive) weight value, whose meaning depends on applications.
        However, most machine learning methods are not able to utilize this extra
        weight information.</p>

    <p>AttributeDataset may also contains meta data such as data name and descriptions.</p>

    <p>Suppose we have an AttributeDataset object, which may be created by a parser. To feed
       the data to a learning algorithm, we need to unwrap the data by the function
       <code>unzip</code> function. If the data also have labels, the function <code>unzipInt</code>
       can be used to return a tuple <code>(x, y)</code>, of which <code>x</code> is an array of
       feature vectors and <code>y</code> is an array of labels. If the response variable is
       real valued in case of regression, <code>unzipDouble</code> can be used.</p>

    <pre class="prettyprint lang-scala"><code>
    val iris = read.arff("data/weka/iris.arff", 4)
    // Get the feature vectors.
    val x = iris.unzip
    // Get the feature vectors and labels.
    val (x, y) = iris.unzipInt
    </code></pre>

    <p>Data scientists generally want to inspect the data and its statistics
        before training advanced models. <code>AttributeDataset</code> provides
        several functions such as <code>head()</code>, <code>tail()</code>,
        <code>range()</code>, <code>summary()</code> for this purpose.
        These functions also return a new <code>AttributeDataset</code>.</p>

    <pre class="prettyprint lang-scala"><code>
smile&gt; iris // or iris.head(10)
res9: AttributeDataset = iris
	class	sepallength	sepalwidth	petallength	petalwidth
[1]	Iris-setosa	5.1000	3.5000	1.4000	0.2000
[2]	Iris-setosa	4.9000	3.0000	1.4000	0.2000
[3]	Iris-setosa	4.7000	3.2000	1.3000	0.2000
[4]	Iris-setosa	4.6000	3.1000	1.5000	0.2000
[5]	Iris-setosa	5.0000	3.6000	1.4000	0.2000
[6]	Iris-setosa	5.4000	3.9000	1.7000	0.4000
[7]	Iris-setosa	4.6000	3.4000	1.4000	0.3000
[8]	Iris-setosa	5.0000	3.4000	1.5000	0.2000
[9]	Iris-setosa	4.4000	2.9000	1.4000	0.2000
[10]	Iris-setosa	4.9000	3.1000	1.5000	0.1000
140 more rows...

smile&gt; iris.tail(10)
res10: AttributeDataset = iris[140, 150]
	class	sepallength	sepalwidth	petallength	petalwidth
[1]	Iris-virginica	6.7000	3.1000	5.6000	2.4000
[2]	Iris-virginica	6.9000	3.1000	5.1000	2.3000
[3]	Iris-virginica	5.8000	2.7000	5.1000	1.9000
[4]	Iris-virginica	6.8000	3.2000	5.9000	2.3000
[5]	Iris-virginica	6.7000	3.3000	5.7000	2.5000
[6]	Iris-virginica	6.7000	3.0000	5.2000	2.3000
[7]	Iris-virginica	6.3000	2.5000	5.0000	1.9000
[8]	Iris-virginica	6.5000	3.0000	5.2000	2.0000
[9]	Iris-virginica	6.2000	3.4000	5.4000	2.3000
[10]	Iris-virginica	5.9000	3.0000	5.1000	1.8000

smile&gt; iris.summary
res11: AttributeDataset = iris Summary
		min	q1	median	mean	q3	max
sepallength		4.3000	5.1000	5.8000	5.8433	6.4000	7.9000
sepalwidth		2.0000	2.8000	3.0000	3.0540	3.3000	4.4000
petallength		1.0000	1.6000	4.4000	3.7587	5.1000	6.9000
petalwidth		0.1000	0.3000	1.3000	1.1987	1.8000	2.5000
    </code></pre>

    <h2 id="data-frame">Data Frame</h2>

    <p>In Scala, we can also wrap <code>AttributeDataset</code> into a
        <code>DataFrame</code>, which provides advanced data manipulation
        functions. For example, we can get a row with the array syntax or
        refer a column by its name</p>

    <pre class="prettyprint lang-scala"><code>
smile&gt; val df = DataFrame(iris)
smile&gt; df(0)
res12: Datum[Array[Double]] = class	sepallength	sepalwidth	petallength	petalwidth
Iris-setosa	5.1000	3.5000	1.4000	0.2000

smile&gt; df.sepallength
res13: AttributeVector = 	sepallength
[1]	5.1000
[2]	4.9000
[3]	4.7000
[4]	4.6000
[5]	5.0000
[6]	5.4000
[7]	4.6000
[8]	5.0000
[9]	4.4000
[10]	4.9000
140 more values...
    </code></pre>

    <p>It is also easy to create a subset of data frame.</p>
    <pre class="prettyprint lang-scala"><code>
smile&gt; df(10,100)
res13: DataFrame = DataFrame(
  iris[10, 100]
	class	sepallength	sepalwidth	petallength	petalwidth
[1]	Iris-setosa	5.4000	3.7000	1.5000	0.2000
[2]	Iris-setosa	4.8000	3.4000	1.6000	0.2000
[3]	Iris-setosa	4.8000	3.0000	1.4000	0.1000
[4]	Iris-setosa	4.3000	3.0000	1.1000	0.1000
[5]	Iris-setosa	5.8000	4.0000	1.2000	0.2000
[6]	Iris-setosa	5.7000	4.4000	1.5000	0.4000
[7]	Iris-setosa	5.4000	3.9000	1.3000	0.4000
[8]	Iris-setosa	5.1000	3.5000	1.4000	0.3000
[9]	Iris-setosa	5.7000	3.8000	1.7000	0.3000
[10]	Iris-setosa	5.1000	3.8000	1.5000	0.3000
80 more rows...
)
    </code></pre>

    <p>Similarly, we can select a few columns to create a new data frame. </p>
    <pre class="prettyprint lang-scala"><code>
smile&gt; df("sepallength", "sepalwidth")
res15: DataFrame = DataFrame(
  iris
	class	sepallength	sepalwidth
[1]	Iris-setosa	5.1000	3.5000
[2]	Iris-setosa	4.9000	3.0000
[3]	Iris-setosa	4.7000	3.2000
[4]	Iris-setosa	4.6000	3.1000
[5]	Iris-setosa	5.0000	3.6000
[6]	Iris-setosa	5.4000	3.9000
[7]	Iris-setosa	4.6000	3.4000
[8]	Iris-setosa	5.0000	3.4000
[9]	Iris-setosa	4.4000	2.9000
[10]	Iris-setosa	4.9000	3.1000
140 more rows...
)
    </code></pre>

    <p>Advanced operations such as <code>exists</code>, <code>forall</code>,
        <code>find</code>, <code>filter</code> are also supported.
        The <code>predicate</code> of these functions expect a <code>Row</code>,
        which has fields <code>x</code> for attribute vector and <code>y</code>
        for responsible variable. If the response variable is a class label,
        the user may use <code>label</code> to access the response variable
        in string label.</p>
    <pre class="prettyprint lang-scala"><code>
smile&gt; df.exists(_.x(0) > 6)
res16: Boolean = true

smile&gt; df.forall(_.x(0) < 10)
res17: Boolean = true

smile&gt; df.find(_.y == 1)
res18: Option[df.Row] = Some(
  class	sepallength	sepalwidth	petallength	petalwidth
Iris-versicolor	7.0000	3.2000	4.7000	1.4000
)

smile&gt; df.find(_.label == "Iris-versicolor") // _.label returns the class label in Strings
res19: Option[df.Row] = Some(
  class	sepallength	sepalwidth	petallength	petalwidth
Iris-versicolor	7.0000	3.2000	4.7000	1.4000
)

smile&gt; df.filter { row => row.x(1) > 3 && row.y != 0 }
res20: DataFrame = DataFrame(
  iris
	class	sepallength	sepalwidth	petallength	petalwidth
[1]	Iris-versicolor	7.0000	3.2000	4.7000	1.4000
[2]	Iris-versicolor	6.4000	3.2000	4.5000	1.5000
[3]	Iris-versicolor	6.9000	3.1000	4.9000	1.5000
[4]	Iris-versicolor	6.3000	3.3000	4.7000	1.6000
[5]	Iris-versicolor	6.7000	3.1000	4.4000	1.4000
[6]	Iris-versicolor	5.9000	3.2000	4.8000	1.8000
[7]	Iris-versicolor	6.0000	3.4000	4.5000	1.6000
[8]	Iris-versicolor	6.7000	3.1000	4.7000	1.5000
[9]	Iris-virginica	6.3000	3.3000	6.0000	2.5000
[10]	Iris-virginica	7.2000	3.6000	6.1000	2.5000
15 more rows...
)
    </code></pre>

    <p>Note that the type of <code>Row.x</code> is <code>Array[Double]</code>.
        If a column is of type <code>String</code> or <code>Date</code>,
        we may refer them by the function <code>string(Int)</code> or
        <code>date(Int)</code> in the predicate.</p>
    <pre class="prettyprint lang-scala"><code>
smile&gt; val df2 = DataFrame(read.arff("data/weka/string.arff"))
df2: DataFrame = DataFrame(
  LCCvsLCSH
		LCC	LCSH
[1]		AG5	Encyclopedias and dictionaries.;Twentieth century.
[2]		AS262	Science -- Soviet Union -- History.
[3]		AE5	Encyclopedias and dictionaries.
[4]		AS281	Astronomy, Assyro-Babylonian.;Moon -- Phases.
[5]		AS281	Astronomy, Assyro-Babylonian.;Moon -- Tables.
)

smile&gt; df2.filter(_.string(0).startsWith("AS"))
res20: DataFrame = DataFrame(
  LCCvsLCSH
		LCC	LCSH
[1]		AS262	Science -- Soviet Union -- History.
[2]		AS281	Astronomy, Assyro-Babylonian.;Moon -- Phases.
[3]		AS281	Astronomy, Assyro-Babylonian.;Moon -- Tables.
)
    </code></pre>

    <p>For data wrangling, the most important functions of <code>DataFrame</code>
        are <code>map</code> and <code>groupBy</code>. There are three flavors of
        <code>map</code>. The first version of <code>map</code> simply treats
        <code>DataFrame</code> as a collection and returns a new collection of
        any type that predicate returns.</p>

    <pre class="prettyprint lang-scala"><code>
smile&gt; df.map { row =>
                val x = new Array[Double](6)
                System.arraycopy(row.x, 0, x, 0, 4)
                x(4) = x(0) * x(1)
                x(5) = x(2) * x(3)
                x
              }
res17: Traversable[Array[Double]] = ArrayBuffer(
  Array(5.1, 3.5, 1.4, 0.2, 17.849999999999998, 0.27999999999999997),
  Array(4.9, 3.0, 1.4, 0.2, 14.700000000000001, 0.27999999999999997),
  Array(4.7, 3.2, 1.3, 0.2, 15.040000000000001, 0.26),
  Array(4.6, 3.1, 1.5, 0.2, 14.26, 0.30000000000000004),
  Array(5.0, 3.6, 1.4, 0.2, 18.0, 0.27999999999999997),
  Array(5.4, 3.9, 1.7, 0.4, 21.060000000000002, 0.68),
  Array(4.6, 3.4, 1.4, 0.3, 15.639999999999999, 0.42),
  Array(5.0, 3.4, 1.5, 0.2, 17.0, 0.30000000000000004),
  Array(4.4, 2.9, 1.4, 0.2, 12.76, 0.27999999999999997),
  Array(4.9, 3.1, 1.5, 0.1, 15.190000000000001, 0.15000000000000002),
  Array(5.4, 3.7, 1.5, 0.2, 19.980000000000004, 0.30000000000000004),
  Array(4.8, 3.4, 1.6, 0.2, 16.32, 0.32000000000000006),
  Array(4.8, 3.0, 1.4, 0.1, 14.399999999999999, 0.13999999999999999),
  Array(4.3, 3.0, 1.1, 0.1, 12.899999999999999, 0.11000000000000001),
  Array(5.8, 4.0, 1.2, 0.2, 23.2, 0.24),
  Array(5.7, 4.4, 1.5, 0.4, 25.080000000000002, 0.6000000000000001),
  Array(5.4, 3.9, 1.3, 0.4, 21.060000000000002, 0.52),
  Array(5.1, 3.5, 1.4, 0.3, 17.849999999999998, 0.42),
  Array(5.7, 3.8, 1.7, 0.3, 21.66, 0.51),
  Array(5.1, 3.8, 1.5, 0.3, 19.38, 0.44999999999999996),
  Array(5.4, 3.4, 1.7, 0.2, 18.36, 0.34),
  Array(5.1, 3.7, 1.5, 0.4, 18.87, 0.6000000000000001),
  Array(4.6, 3.6, 1.0, 0.2, 16.56, 0.2),
  Array(5.1, 3.3, 1.7, 0.5, 16.83, 0.85),
...
    </code></pre>

    <p>Optionally, the <code>map</code> functions may take an array of <code>Attribute</code>
        as parameter and return a new <code>DataFrame</code>.</p>

    <pre class="prettyprint lang-scala"><code>
smile&gt; val attr = new Array[Attribute](6)
          System.arraycopy(iris.attributes, 0, attr, 0, 4)
          attr(4) = new NumericAttribute("sepallength * sepalwidth")
          attr(5) = new NumericAttribute("petallength * petalwidth")

          df.map(attr) { row =>
            val x = new Array[Double](6)
            System.arraycopy(row.x, 0, x, 0, 4)
            x(4) = x(0) * x(1)
            x(5) = x(2) * x(3)
            x
          }
res23: DataFrame = DataFrame(
  iris
		sepallength	sepalwidth	petallength	petalwidth	sepallength * sepalwidth	petallength * petalwidth
[1]		5.1000	3.5000	1.4000	0.2000	17.8500	0.2800
[2]		4.9000	3.0000	1.4000	0.2000	14.7000	0.2800
[3]		4.7000	3.2000	1.3000	0.2000	15.0400	0.2600
[4]		4.6000	3.1000	1.5000	0.2000	14.2600	0.3000
[5]		5.0000	3.6000	1.4000	0.2000	18.0000	0.2800
[6]		5.4000	3.9000	1.7000	0.4000	21.0600	0.6800
[7]		4.6000	3.4000	1.4000	0.3000	15.6400	0.4200
[8]		5.0000	3.4000	1.5000	0.2000	17.0000	0.3000
[9]		4.4000	2.9000	1.4000	0.2000	12.7600	0.2800
[10]		4.9000	3.1000	1.5000	0.1000	15.1900	0.1500
140 more rows...
)
    </code></pre>

    <p>We can pass an additional response attribute too:</p>
    <pre class="prettyprint lang-scala"><code>
smile&gt; df.map(attr, iris.responseAttribute) { row =>
            val x = new Array[Double](6)
            System.arraycopy(row.x, 0, x, 0, 4)
            x(4) = x(0) * x(1)
            x(5) = x(2) * x(3)
            (x, row.y)
          }
res24: DataFrame = DataFrame(
  iris
	class	sepallength	sepalwidth	petallength	petalwidth	sepallength * sepalwidth	petallength * petalwidth
[1]	Iris-setosa	5.1000	3.5000	1.4000	0.2000	17.8500	0.2800
[2]	Iris-setosa	4.9000	3.0000	1.4000	0.2000	14.7000	0.2800
[3]	Iris-setosa	4.7000	3.2000	1.3000	0.2000	15.0400	0.2600
[4]	Iris-setosa	4.6000	3.1000	1.5000	0.2000	14.2600	0.3000
[5]	Iris-setosa	5.0000	3.6000	1.4000	0.2000	18.0000	0.2800
[6]	Iris-setosa	5.4000	3.9000	1.7000	0.4000	21.0600	0.6800
[7]	Iris-setosa	4.6000	3.4000	1.4000	0.3000	15.6400	0.4200
[8]	Iris-setosa	5.0000	3.4000	1.5000	0.2000	17.0000	0.3000
[9]	Iris-setosa	4.4000	2.9000	1.4000	0.2000	12.7600	0.2800
[10]	Iris-setosa	4.9000	3.1000	1.5000	0.1000	15.1900	0.1500
140 more rows...
)
    </code></pre>

    <p>For <code>groupBy</code>, the user may provide one more more
        column names, on which the grouping operation is based.
        The user may also provide a generic function that returns
        the key of groups.</p>

    <pre class="prettyprint lang-scala"><code>
smile&gt; val labor = read.arff("data/weka/labor.arff", 16)
smile&gt; labor.groupBy("cost-of-living-adjustment")
res1: Map[Double, DataFrame] = Map(
  0.0 -> DataFrame(
    labor-neg-data
	class	duration	wage-increase-first-year	wage-increase-second-year	wage-increase-third-year	cost-of-living-adjustment	working-hours	pension	standby-pay	shift-differential	education-allowance	statutory-holidays	vacation	longterm-disability-assistance	contribution-to-dental-plan	bereavement-assistance	contribution-to-health-plan
[1]	good	1.0000	5.7000	NaN	NaN	none	40.0000	empl_contr	NaN	4.0000	null	11.0000	generous	yes	full	null	null
[2]	good	3.0000	3.5000	4.0000	4.6000	none	36.0000	null	NaN	3.0000	null	13.0000	generous	null	null	yes	full
[3]	bad	2.0000	3.5000	4.0000	NaN	none	40.0000	null	NaN	2.0000	no	10.0000	below_average	no	half	null	half
[4]	good	1.0000	3.0000	NaN	NaN	none	36.0000	null	NaN	10.0000	no	11.0000	generous	null	null	null	null
[5]	good	2.0000	4.5000	4.0000	NaN	none	37.0000	empl_contr	NaN	NaN	null	11.0000	average	null	full	yes	null
[6]	bad	1.0000	2.0000	NaN	NaN	none	38.0000	none	NaN	NaN	yes	11.0000	average	no	none	no	none
[7]	bad	3.0000	2.0000	2.0000	2.0000	none	40.0000	none	NaN	NaN	null	10.0000	below_average	null	half	yes	full
[8]	good	2.0000	3.0000	3.0000	NaN	none	33.0000	null	NaN	NaN	yes	12.0000	generous	null	null	yes	full
[9]	good	2.0000	5.0000	4.0000	NaN	none	37.0000	null	NaN	5.0000	no	11.0000	below_average	yes	full	yes	full
[10]	good	3.0000	4.5000	4.5000	5.0000	none	40.0000	null	NaN	NaN	no	11.0000	average	null	half	null	null
12 more rows...
  ),
  1.0 -> DataFrame(
    labor-neg-data
	class	duration	wage-increase-first-year	wage-increase-second-year	wage-increase-third-year	cost-of-living-adjustment	working-hours	pension	standby-pay	shift-differential	education-allowance	statutory-holidays	vacation	longterm-disability-assistance	contribution-to-dental-plan	bereavement-assistance	contribution-to-health-plan
[1]	good	3.0000	3.5000	4.0000	5.1000	tcf	37.0000	null	NaN	4.0000	null	13.0000	generous	null	full	yes	full
[2]	good	2.0000	4.0000	5.0000	NaN	tcf	35.0000	null	13.0000	5.0000	null	15.0000	generous	null	null	null	null
[3]	good	3.0000	3.5000	4.0000	4.6000	tcf	27.0000	null	NaN	NaN	null	NaN	null	null	null	null	null
[4]	good	2.0000	4.5000	4.5000	NaN	tcf	NaN	null	NaN	NaN	yes	10.0000	below_average	yes	none	null	half
...
    </code></pre>

    <h2 id="sparse">Sparse Dataset</h2>

    <p>The feature vectors could be very sparse. To save space, <a href="api/java/smile/data/SparseDataset.html">SparseDataset</a>
        stores data in a list of lists (LIL) sparse matrix format. SparseDataset stores one list
        per row, where each entry stores a column index and value. Typically, these entries
        are kept sorted by column index for faster lookup.</p>

    <p>SparseDataset is often used to construct the data matrix. Once the matrix is constructed,
        it is typically converted to a format, such as <a href="api/java/smile/math/matrix/SparseMatrix.html">Harwell-Boeing</a>
        column-compressed sparse matrix format, which is more efficient for matrix operations.</p>

    <p>The class <a href="api/java/smile/data/BinarySparseDataset.html">BinarySparseDataset</a> is more efficient for
        binary sparse data. In BinarySparseDataset, each item is stored as an integer array, which are
        the indices of nonzero elements in ascending order.</p>

    <h2 id="parser">Parsers</h2>

    <p>Smile provides a couple of parsers for popular data formats, such as Weka's ARFF files,
        LibSVM's file format, delimited text files, and binary sparse data. We will demonstrate
        these parsers with the sample data in the <code>data</code> directory. In Scala API, the
        parsing functions are in the <code>smile.read</code> object.</p>

    <h3 id="read.arff">Weka ARFF</h3>
    <p><a href="http://www.cs.waikato.ac.nz/ml/weka/arff.html">Weka ARFF (attribute relation file format)</a>
        is an ASCII text file format that is essentially a CSV file with a header that describes the meta data.
        ARFF was developed for use in the <a href="http://www.cs.waikato.ac.nz/ml/weka/">Weka</a> machine learning software.</p>

    <p>A dataset is firstly described, beginning with the name of the dataset (or the relation in ARFF terminology).
        Each of the variables (or attribute in ARFF terminology) used to describe the observations is then identified,
        together with their data type, each definition on a single line. The actual observations are then listed,
        each on a single line, with fields separated by commas, much like a CSV file.</p>

    <p>Missing values in an ARFF dataset are identified using the question mark '?'.
        Comments can be included in the file, introduced at the beginning of a line with a '%',
        whereby the remainder of the line is ignored.</p>

    <p>A significant advantage of the ARFF data file over the CSV data file is the meta data information.
        Also, the ability to include comments ensure we can record extra information about the data set,
        including how it was derived, where it came from, and how it might be cited.</p>

    <p>To read an ARFF file, <code>smile.io</code> has the function</p>

    <pre class="prettyprint lang-scala"><code>
    def read.arff(file: String, responseIndex: Int = -1): AttributeDataset
    </code></pre>

    <p>where <code>responseIndex</code> is the column index (starting at 0) of response variable.
        The default value -1 means no response variable. In the directory <code>data/weka</code>,
        we have many sample ARFF files. Try the following code to load a dataset:</p>

    <pre class="prettyprint lang-scala"><code>
    val data = read.arff("data/weka/iris.arff", 4)
    </code></pre>

    <h3 id="read.libsvm">LibSVM</h3>
    <p>LibSVM is a very fast and popular library for support vector machines.
        LibSVM uses a sparse format where zero values do not need to be stored.
        Each line of a libsvm file is in the format:</p>
    <pre><code>
    &lt;label&gt; &lt;index1&gt;:&lt;value1&gt; &lt;index2&gt;:&lt;value2&gt; ...
    </code></pre>
    <p>where &lt;label&gt; is the target value of the training data.
        For classification, it should be an integer which identifies a class
        (multi-class classification is supported). For regression, it's any real
        number. For one-class SVM, it's not used so can be any number.
        &lt;index&gt; is an integer starting from 1, and &lt;value&gt;
        is a real number. The indices must be in an ascending order. The labels in
        the testing data file are only used to calculate accuracy or error. If they
        are unknown, just fill this column with a number.</p>

    <p>To read a libsvm file, <code>smile.io</code> has the function</p>

    <pre class="prettyprint lang-scala"><code>
    def read.libsvm(file: String): SparseDataset
    </code></pre>

    <p>Although libsvm employs a sparse format, most libsvm files contain dense data.
        Therefore, Smile also provides helper functions to convert
        it to dense arrays.</p>

    <pre class="prettyprint lang-scala"><code>
    val data = read.libsvm("data/libsvm/glass.txt")
    val (x, y) = data.unzipInt
    </code></pre>

    <p>In case of truly sparse libsvm data, we can convert it to <code>SparseMatrix</code>
        for more efficient matrix computation.</p>
    <pre class="prettyprint lang-scala"><code>
    smile&gt; val sparse = data.toSparseMatrix
    sparse: smile.math.matrix.SparseMatrix = smile.math.matrix.SparseMatrix@73ad7e90
    </code></pre>

    <h3 id="read.csv">Delimited Text and CSV</h3>
    <p>The delimited text files are widely used in machine learning research community.
        The comma-separated values (CSV) file is a special case. Smile provides flexible
        parser for them.</p>

    <pre class="prettyprint lang-scala"><code>
    def read.table(file: String, response: Option[(Attribute, Int)] = None, delimiter: String = "\\s+", comment: String = "%", missing: String = "?", header: Boolean = false, rowNames: Boolean = false): AttributeDataset

    def read.csv(file: String, response: Option[(Attribute, Int)] = None, comment: String = "%", missing: String = "?", header: Boolean = false, rowNames: Boolean = false): AttributeDataset
    </code></pre>

    <p>By default, the parser <code>read.table</code> expects a white-space-separated-values file.
        Each line in the file corresponds to a row in the table. Within a line,
        fields are separated by white spaces, each field belonging to one table column.
        The function <code>read.csv</code> is a shortcut to <code>read.table</code> with comma ',' as the delimiter
        character. The file may contain comment lines (starting with '%') and missing values
        (indicated by placeholder '?'), which both can be parameterized. Optionally, the file may also have
        a header (e.g. the first row is column names) and the first column may be row id/names.</p>

    <pre class="prettyprint lang-scala"><code>
    val label = new NominalAttribute("class")
    val data = read.table("data/usps/zip.train", Some((label, 0)))
    </code></pre>

    <h3 id="microarray">Microarray</h3>
    <p>Smile can parse several popular microarray data format. A DNA microarray is a
        collection of microscopic DNA spots attached to a solid surface. Scientists
        use DNA microarrays to measure the expression levels of large numbers of
        genes simultaneously or to genotype multiple regions of a genome.</p>

    <p>In particular, we support <a href="api/java/smile/data/parser/microarray/GCTParser.html">GCT</a>,
        <a href="api/java/smile/data/parser/microarray/PCLParser.html">PCL</a>,
        <a href="api/java/smile/data/parser/microarray/RESParser.html">RES</a>,
        and <a href="api/java/smile/data/parser/microarray/TXTParser.html">TXT</a> file formats.</p>

    <pre class="prettyprint lang-scala"><code>
    def read.gct(file: String): AttributeDataset

    def read.pcl(file: String): AttributeDataset

    def read.res(file: String): AttributeDataset

    def read.txt(file: String): AttributeDataset
    </code></pre>

    <h3 id="sparse-format">Coordinate Triple Tuple List</h3>

    <p>The function <code>read.coo</code> can read sparse data in coordinate triple tuple list format.</p>

    <pre class="prettyprint lang-scala"><code>
    def read.coo(file: String, arrayIndexOrigin: Int = 0): SparseDataset
    </code></pre>

    <p>where <code>arrayIndexOrigin</code> is the starting index of array. By default, it is
        0 as in C/C++ and Java. But it could be 1 to parse data produced
        by other programming language such as Fortran.</p>

    <p>The coordinate file stores a list of (row, column, value) tuples:</p>
    <pre>
    instanceID attributeID value
    instanceID attributeID value
    instanceID attributeID value
    instanceID attributeID value
    ...
    instanceID attributeID value
    instanceID attributeID value
    instanceID attributeID value
    </pre>

    <p>Ideally, the entries are sorted (by row index, then column index) to improve
        random access times. This format is good for incremental matrix
        construction.</p>

    <p>Optionally, there may be 2 header lines</p>

    <pre>
    D    // The number of instances
    W    // The number of attributes
    </pre>

    <p>or 3 header lines</p>

    <pre>
    D    // The number of instances
    W    // The number of attributes
    N    // The total number of nonzero items in the dataset.
    </pre>
    <p>These header lines will be ignored.</p>

    <p>The sample data <code>data/text/kos.txt</code> is in the coordinate format.</p>

    <pre class="prettyprint lang-scala"><code>
    val data = read.coo("data/text/kos.txt")
    </code></pre>

    <h3 id="Harwell-Boeing">Harwell-Boeing Column-Compressed Sparse Matrix</h3>

    <p>The function <code>read.hb</code> can read sparse matrix in Harwell-Boeing column-compressed format.</p>

    <pre class="prettyprint lang-scala"><code>
    def read.hb(file: String): SparseMatrix
    </code></pre>

    <p>In the directory <code>data/matrix</code>, there are several sample files in
       the Harwell-Boeing format.</p>

    <pre class="prettyprint lang-scala"><code>
    val data = read.hb("data/matrix/08blocks.txt")
    </code></pre>

    <p>In Harwell-Boeing column-compressed sparse matrix file, nonzero values are stored in an array
        (top-to-bottom, then left-to-right-bottom). The row indices corresponding to
        the values are also stored. Besides, a list of pointers are indexes where
        each column starts. This class supports two formats for Harwell-Boeing files.
        The simple one is organized as follows:</p>

    <p>The first line contains three integers, which are the number of rows,
        the number of columns, and the number of nonzero entries in the matrix.</p>

    <p>Following the first line, there are m + 1 integers that are the indices of
        columns, where m is the number of columns. Then there are n integers that
        are the row indices of nonzero entries, where n is the number of nonzero
        entries. Finally, there are n float numbers that are the values of nonzero
        entries.</p>

    <p>The second format is more complicated and powerful, called Harwell-Boeing Exchange Format.
        For details, see <a href="http://people.sc.fsu.edu/~jburkardt/data/hb/hb.html">http://people.sc.fsu.edu/~jburkardt/data/hb/hb.html</a>.
        Note that our implementation supports only real-valued matrix and we ignore
        the optional right hand side vectors.</p>

    <h3 id="wireframe">Wireframe</h3>
    <p>Smile can parse 3D wireframe models in Wavefront OBJ files.</p>

    <pre class="prettyprint lang-scala"><code>
    def read.wavefront(file: String): (Array[Array[Double]], Array[Array[Int]])
    </code></pre>

    <p>In the directory <code>data/wireframe</code>, there is a teapot wireframe model. In the
        next section, we will learn how to visualize the 3D wireframe models.</p>

    <pre class="prettyprint lang-scala"><code>
    val (vertices, edges) = read.wavefront("data/wireframe/teapot.obj")
    </code></pre>

    <h2 id="export">Export Data and Models</h2>

    <p>To serialize a model, you may use</p>
    <pre class="prettyprint lang-scala"><code>
    write(model, file)
    </code></pre>
    <p>This method is in the Scala API <code>smile.write</code> object and serialize the model to Java
        serialization format. This is handy if you want to use a model in Spark.</p>

    <p>Alternatively, you can also use</p>
    <pre class="prettyprint lang-scala"><code>
    write.xstream(model, file)
    </code></pre>

    <p>which uses XStream library to serialize the model (actually any objects) to XML file.</p>

    <p>To read the model back, you can use <code>read(file)</code> or <code>read.xstream(file)</code>, correspondingly.</p>

    <p>You can also save an <code>AttributeDataset</code> to an ARFF file with the method
        <code>write.arff(data, file)</code>. The ARFF file keeps the data type information.
        If you prefer the plain csv text file, you may use the methods <code>write.csv(data, file)</code> or
        <code>write.table(data, file, "delimiter")</code>, which save a generic two dimensional array
        with comma or customized delimiter. To save one dimensional array, simply call
        <code>write(array, file)</code>.</p>

    <div id="btnv">
        <span class="btn-arrow-left">&larr; &nbsp;</span>
        <a class="btn-prev-text" href="shell.html" title="Previous Section: Shell"><span>Shell</span></a>
        <a class="btn-next-text" href="visualization.html" title="Next Section: Classification"><span>Visualization</span></a>
        <span class="btn-arrow-right">&nbsp;&rarr;</span>
    </div>
</div>

<script type="text/javascript">
    $('#toc').toc({exclude: 'h1, h5, h6', context: '', autoId: true, numerate: false});
</script>
